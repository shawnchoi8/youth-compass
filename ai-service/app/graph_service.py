"""
LangGraph ê¸°ë°˜ ì²­ë…„ ê¸ˆìœµ/ì£¼íƒ ì •ì±… ì±—ë´‡ ì›Œí¬í”Œë¡œìš°
ChromaDB PDF ê²€ìƒ‰ â†’ ê´€ë ¨ì„± ì²´í¬ â†’ ì›¹ ê²€ìƒ‰ (í•„ìš”ì‹œ) â†’ ë‹µë³€ ìƒì„±
"""

from typing import Annotated, TypedDict, Optional
from langgraph.graph import END, StateGraph
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langchain_upstage import ChatUpstage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from tavily import TavilyClient
from app.config import settings
from app.rag_service import rag_service
import logging
import json

logger = logging.getLogger(__name__)


# GraphState ì •ì˜
class GraphState(TypedDict):
    """ê·¸ë˜í”„ ìƒíƒœ"""
    question: Annotated[str, "Question"]  # ì‚¬ìš©ì ì§ˆë¬¸
    context: Annotated[str, "Context"]  # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸
    answer: Annotated[str, "Answer"]  # ìƒì„±ëœ ë‹µë³€
    messages: Annotated[list, add_messages]  # ëŒ€í™” íˆìŠ¤í† ë¦¬
    relevance: Annotated[str, "Relevance"]  # ê´€ë ¨ì„± ì²´í¬ ê²°ê³¼ (yes/no)
    search_source: Annotated[str, "SearchSource"]  # ì •ë³´ ì¶œì²˜ (pdf/web)
    user_profile: Annotated[dict, "UserProfile"]  # ì‚¬ìš©ì í”„ë¡œí•„


# ì²­ë…„ ì •ì±… ì „ë¬¸ í”„ë¡¬í”„íŠ¸
YOUTH_POLICY_PROMPT = ChatPromptTemplate.from_messages([
    (
        "system",
        """ë‹¹ì‹ ì€ ì²­ë…„ ê¸ˆìœµ ë° ì£¼íƒ ì •ì±… ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

**ì—­í• **
- ì²­ë…„ë“¤ì˜ ê¸ˆìœµ ë° ì£¼íƒ ê´€ë ¨ ê³ ë¯¼ì„ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ í•´ê²°í•´ì£¼ì„¸ìš”.
- ë³µì¡í•œ ì •ì±…ì„ ì‰½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
- êµ¬ì²´ì ì¸ ì‹ ì²­ ì¡°ê±´, ì ˆì°¨, í•„ìš” ì„œë¥˜ë¥¼ ì•ˆë‚´í•´ì£¼ì„¸ìš”.

**ë‹µë³€ ì›ì¹™**
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ì •ë³´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.
2. ì‹ ì²­ ìê²©, ëŒ€ì¶œ í•œë„, ê¸ˆë¦¬ ë“± í•µì‹¬ ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ ì•ˆë‚´í•©ë‹ˆë‹¤.
3. ì—¬ëŸ¬ ì •ì±…ì´ ìˆë‹¤ë©´ ë¹„êµí•˜ì—¬ ìµœì ì˜ ì„ íƒì„ ë„ì™€ì¤ë‹ˆë‹¤.
4. ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ì•Šê³  í™•ì¸ì´ í•„ìš”í•˜ë‹¤ê³  ì•ˆë‚´í•©ë‹ˆë‹¤.
5. ì¹œê·¼í•˜ê³  ê³µê°í•˜ëŠ” í†¤ìœ¼ë¡œ ëŒ€í™”í•©ë‹ˆë‹¤.

**ë‹µë³€ í˜•ì‹**
- í•µì‹¬ ë‚´ìš©ì„ ë¨¼ì € ì œì‹œí•˜ê³ , ìƒì„¸ ì •ë³´ë¥¼ ì´ì–´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤.
- ì¡°ê±´ì´ ìˆëŠ” ê²½ìš° ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì—¬ ì„¤ëª…í•©ë‹ˆë‹¤.
- í•„ìš”ì‹œ ë‹¨ê³„ë³„ë¡œ ì •ë¦¬í•˜ì—¬ ì•ˆë‚´í•©ë‹ˆë‹¤.

**ì œê³µëœ ì»¨í…ìŠ¤íŠ¸**
{context}

**ì‚¬ìš©ì í”„ë¡œí•„(ìˆìœ¼ë©´ ë°˜ì˜)**
{user_profile}

**ì´ì „ ëŒ€í™” ë‚´ì—­**
{chat_history}
""",
    ),
    ("human", "{question}"),
])


class GraphService:
    """LangGraph ê¸°ë°˜ ì±—ë´‡ ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.llm = None
        self.youth_policy_chain = None
        self.tavily_client = None
        self.app = None
        self.memory = MemorySaver()
        self._initializing = False
        self._initialized = False
        
        # ì´ˆê¸°í™”ëŠ” ë‚˜ì¤‘ì— (ì„œë²„ ì‹œì‘ í›„)
        # self._initialize()  # ì£¼ì„ ì²˜ë¦¬
    
    def initialize(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ë™ê¸°)"""
        if self._initializing or self._initialized:
            return
        self._initializing = True
        try:
            self._initialize()
            self._initialized = True
            logger.info("GraphService ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"GraphService ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
            self._initialized = False
        finally:
            self._initializing = False
    
    def _initialize(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ë‚´ë¶€ ë¡œì§"""
        try:
            # Upstage Solar LLM ì´ˆê¸°í™” (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
            if settings.upstage_api_key:
                self.llm = ChatUpstage(
                    model=settings.upstage_model,
                    temperature=settings.temperature,
                    api_key=settings.upstage_api_key,
                    streaming=True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
                )
                self.youth_policy_chain = YOUTH_POLICY_PROMPT | self.llm | StrOutputParser()
                logger.info("Upstage Solar LLM ì´ˆê¸°í™” ì™„ë£Œ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)")
            else:
                logger.warning("UPSTAGE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            # Tavily ì›¹ ê²€ìƒ‰ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            if settings.tavily_api_key:
                self.tavily_client = TavilyClient(api_key=settings.tavily_api_key)
                logger.info("Tavily ì›¹ ê²€ìƒ‰ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                logger.warning("TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            # LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì¶•
            self._build_graph()
            
        except Exception as e:
            logger.error(f"GraphService ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _build_graph(self):
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì¶•"""
        if not self.llm:
            logger.warning("LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # ê·¸ë˜í”„ ìƒì„±
        workflow = StateGraph(GraphState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("retrieve", self._retrieve_document)
        workflow.add_node("relevance_check", self._relevance_check)
        workflow.add_node("web_search", self._web_search)
        workflow.add_node("llm_answer", self._llm_answer)
        
        # ì—£ì§€ ì¶”ê°€
        workflow.add_edge("retrieve", "relevance_check")
        workflow.add_conditional_edges(
            "relevance_check",
            self._is_relevant,
            {
                "relevant": "llm_answer",
                "not_relevant": "web_search"
            }
        )
        workflow.add_edge("web_search", "llm_answer")
        workflow.add_edge("llm_answer", END)
        
        # ì§„ì…ì  ì„¤ì •
        workflow.set_entry_point("retrieve")
        
        # ê·¸ë˜í”„ ì»´íŒŒì¼
        self.app = workflow.compile(checkpointer=self.memory)
        logger.info("LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì¶• ì™„ë£Œ")
    
    def _retrieve_document(self, state: GraphState) -> GraphState:
        """1. PDF ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ"""
        question = state["question"]
        logger.info(f"PDF ë¬¸ì„œ ê²€ìƒ‰: {question[:50]}...")
        
        # RAG ì„œë¹„ìŠ¤ë¡œ ë¬¸ì„œ ê²€ìƒ‰
        retriever = rag_service.get_retriever()
        if retriever:
            try:
                retrieved_docs = retriever.invoke(question)
                context = rag_service.format_docs(retrieved_docs)
                
                if context:
                    logger.info(f"{len(retrieved_docs)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬")
                else:
                    logger.info("ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•¨")
                
                return GraphState(context=context, search_source="pdf")
            except Exception as e:
                logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                return GraphState(context="", search_source="pdf")
        else:
            logger.warning("Retrieverê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return GraphState(context="", search_source="pdf")
    
    def _relevance_check(self, state: GraphState) -> GraphState:
        """2. ê´€ë ¨ì„± ì²´í¬ ë…¸ë“œ (LLM ê¸°ë°˜)"""
        # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ê´€ë ¨ì„± ì—†ìŒ
        context = state.get("context", "")
        if not context or context.strip() == "":
            logger.info("ê´€ë ¨ì„± ì²´í¬: NO (ë¬¸ì„œ ì—†ìŒ)")
            return GraphState(relevance="no")
        
        question = state["question"]
        
        # LLMì„ ì‚¬ìš©í•œ ì •êµí•œ ê´€ë ¨ì„± ì²´í¬
        relevance_prompt = f"""ë‹¹ì‹ ì€ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì§ˆë¬¸: {question}

ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©:
{context[:1000]}

ìœ„ ë¬¸ì„œê°€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ë° ìœ ìš©í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆê¹Œ?

ê·œì¹™:
- ë¬¸ì„œ ë‚´ìš©ì´ ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆìœ¼ë©´ "YES"
- ë¬¸ì„œ ë‚´ìš©ì´ ì§ˆë¬¸ê³¼ ì „í˜€ ê´€ë ¨ì´ ì—†ìœ¼ë©´ "NO"
- ë‹¨ìˆœíˆ í‚¤ì›Œë“œê°€ ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì‹¤ì§ˆì ìœ¼ë¡œ ë‹µë³€ì— ë„ì›€ì´ ë˜ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”

ë‹µë³€ì€ ë°˜ë“œì‹œ "YES" ë˜ëŠ” "NO" ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        try:
            response = self.llm.invoke(relevance_prompt)
            result = response.content.strip().upper()
            
            relevance = "yes" if "YES" in result else "no"
            logger.info(f"ê´€ë ¨ì„± ì²´í¬: {relevance.upper()} (LLM íŒë‹¨: {result[:20]})")
            
        except Exception as e:
            logger.error(f"ê´€ë ¨ì„± ì²´í¬ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ 'yes' ì‚¬ìš©")
            relevance = "yes"
        
        return GraphState(relevance=relevance)
    
    def _web_search(self, state: GraphState) -> GraphState:
        """3. ì›¹ ê²€ìƒ‰ ë…¸ë“œ"""
        question = state["question"]
        logger.info(f"ì›¹ ê²€ìƒ‰: {question[:50]}...")
        
        if not self.tavily_client:
            logger.warning("Tavily í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return GraphState(
                context="ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                search_source="web"
            )
        
        try:
            # ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” (ì²­ë…„ ì •ì±… í‚¤ì›Œë“œ ì¶”ê°€)
            enhanced_query = f"ì²­ë…„ {question}" if "ì²­ë…„" not in question else question
            
            # Tavily ê²€ìƒ‰ ìˆ˜í–‰
            search_results = self.tavily_client.search(
                query=enhanced_query,
                max_results=5
            )
            
            # ê²°ê³¼ í¬ë§·íŒ…
            context = ""
            if search_results and "results" in search_results:
                for result in search_results["results"][:3]:
                    context += f"{result.get('content', '')}\n\n"
            
            logger.info("ì›¹ ê²€ìƒ‰ ì™„ë£Œ")
            return GraphState(context=context, search_source="web")
            
        except Exception as e:
            logger.error(f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return GraphState(
                context=f"ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                search_source="web"
            )
    
    def _llm_answer(self, state: GraphState) -> GraphState:
        """4. ë‹µë³€ ìƒì„± ë…¸ë“œ"""
        question = state["question"]
        context = state.get("context", "")
        
        logger.info("ë‹µë³€ ìƒì„± ì¤‘...")
        
        if not self.youth_policy_chain:
            return GraphState(
                answer="AI ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                messages=[("user", question), ("assistant", "ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì˜¤ë¥˜")]
            )
        
        try:
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
            chat_history = ""
            messages = state.get("messages", [])
            if messages:
                for msg in messages[-6:]:  # ìµœê·¼ 3í„´
                    if isinstance(msg, tuple) and len(msg) == 2:
                        role, content = msg
                        chat_history += f"{role}: {content}\n"
                    elif hasattr(msg, 'type') and hasattr(msg, 'content'):
                        chat_history += f"{msg.type}: {msg.content}\n"
            
            # ë‹µë³€ ìƒì„±
            response = self.youth_policy_chain.invoke({
                "question": question,
                "context": context,
                "chat_history": chat_history,
                "user_profile": state.get("user_profile", {})
            })
            
            # ì •ë³´ ì¶œì²˜ ì•ˆë‚´ ì¶”ê°€
            source = state.get("search_source", "unknown")
            source_text = {
                "pdf": "\n\nğŸ“„ *[ì¶œì²˜: ì—…ë¡œë“œëœ ì •ì±… ë¬¸ì„œ]*",
                "web": "\n\nğŸŒ *[ì¶œì²˜: ì›¹ ê²€ìƒ‰ ê²°ê³¼ - ìµœì‹  ì •ë³´ì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ê³µì‹ ì‚¬ì´íŠ¸ì—ì„œ í™•ì¸ì„ ê¶Œì¥í•©ë‹ˆë‹¤]*"
            }.get(source, "")
            
            final_answer = f"{response}{source_text}"
            
            logger.info("ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
            return GraphState(
                answer=final_answer,
                messages=[("user", question), ("assistant", final_answer)]
            )
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            error_msg = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            return GraphState(
                answer=error_msg,
                messages=[("user", question), ("assistant", error_msg)]
            )
    
    def _is_relevant(self, state: GraphState) -> str:
        """ê´€ë ¨ì„± ë¼ìš°íŒ… í•¨ìˆ˜"""
        return "relevant" if state.get("relevance") == "yes" else "not_relevant"
    
    async def ask(
        self,
        question: str,
        thread_id: str,
        user_profile: Optional[dict] = None
    ) -> dict:
        """
        ì§ˆë¬¸í•˜ê³  ë‹µë³€ ë°›ê¸°
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            thread_id: ëŒ€í™” ì„¸ì…˜ ID
            user_profile: ì‚¬ìš©ì í”„ë¡œí•„ (ì„ íƒ)
            
        Returns:
            ë‹µë³€ ë° ìƒíƒœ ì •ë³´
        """
        if not self.app:
            return {
                "answer": "AI ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. UPSTAGE_API_KEYë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
                "search_source": "error"
            }
        
        try:
            # ì…ë ¥ ì¤€ë¹„
            inputs = GraphState(
                question=question,
                user_profile=(user_profile or {})
            )
            
            # ì„¤ì •
            config = {"configurable": {"thread_id": thread_id}}
            
            # ì‹¤í–‰
            result = await self.app.ainvoke(inputs, config)
            
            return {
                "answer": result.get("answer", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."),
                "search_source": result.get("search_source", "unknown"),
                "context": result.get("context", "")
            }
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "answer": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "search_source": "error"
            }
    
    async def stream_ask(
        self,
        question: str,
        thread_id: str,
        user_profile: Optional[dict] = None
    ):
        """
        ì§ˆë¬¸í•˜ê³  ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹µë³€ ë°›ê¸°
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            thread_id: ëŒ€í™” ì„¸ì…˜ ID
            user_profile: ì‚¬ìš©ì í”„ë¡œí•„ (ì„ íƒ)
            
        Yields:
            ë‹µë³€ ì²­í¬ ë° ë©”íƒ€ë°ì´í„°
        """
        if not self.app:
            yield {
                "type": "error",
                "content": "AI ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
            return
        
        try:
            logger.info(f"ìŠ¤íŠ¸ë¦¬ë° ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: {question[:50]}...")
            
            # 1. ë¬¸ì„œ ê²€ìƒ‰ ë‹¨ê³„
            yield {"type": "status", "content": "ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."}
            documents = rag_service.search(question, k=3)
            
            context = "\n\n".join([
                f"ë¬¸ì„œ {i+1}:\n{doc['content']}" 
                for i, doc in enumerate(documents)
            ]) if documents else ""
            
            # 2. ê´€ë ¨ì„± ì²´í¬ (ì²­ë…„ ì •ì±… ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸)
            youth_keywords = ["ì²­ë…„", "ì£¼íƒ", "ì „ì„¸", "ëŒ€ì¶œ", "ê¸ˆìœµ", "ì§€ì›", "ì •ì±…", "ì„ëŒ€", "ì¼ìë¦¬"]
            is_relevant = any(keyword in question for keyword in youth_keywords) or \
                          (context and any(keyword in context[:500] for keyword in youth_keywords))
            
            # 3. ê´€ë ¨ì„±ì´ ì—†ê±°ë‚˜ ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ë©´ ì›¹ ê²€ìƒ‰
            if not is_relevant or not context or len(context) < 100:
                yield {"type": "status", "content": "ì›¹ ê²€ìƒ‰ ì¤‘..."}
                search_source = "web"
                # ì›¹ ê²€ìƒ‰
                if self.tavily_client:
                    try:
                        web_results = self.tavily_client.search(
                            query=question,
                            max_results=3
                        )
                        if web_results and "results" in web_results:
                            context = "\n\n".join([
                                f"[{result.get('title', 'Unknown')}]\n{result.get('content', '')}"
                                for result in web_results.get("results", [])
                            ])
                        else:
                            context = "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    except Exception as e:
                        logger.warning(f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                        context = "ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                else:
                    context = "ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            else:
                # PDF ê²€ìƒ‰ ê²°ê³¼ê°€ ê´€ë ¨ ìˆìœ¼ë©´ PDF ì‚¬ìš©
                search_source = "pdf"
            
            # 3. ë©”íƒ€ë°ì´í„° ì „ì†¡
            yield {
                "type": "metadata",
                "search_source": search_source,
                "context_length": len(context)
            }
            
            # 4. ë‹µë³€ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)
            yield {"type": "status", "content": "ë‹µë³€ ìƒì„± ì¤‘..."}
            
            if not self.llm:
                yield {
                    "type": "error",
                    "content": "AI ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                }
                return
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
            chat_history = ""
            user_profile_text = ""
            if user_profile:
                user_profile_text = f"\nì‚¬ìš©ì í”„ë¡œí•„: {json.dumps(user_profile, ensure_ascii=False)}"
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            messages = YOUTH_POLICY_PROMPT.format_messages(
                question=question,
                context=context if context else "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                chat_history=chat_history,
                user_profile=user_profile_text
            )
            
            # ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± (LLM ì§ì ‘ ì‚¬ìš©)
            full_response = ""
            try:
                async for chunk in self.llm.astream(messages):
                    if hasattr(chunk, 'content') and chunk.content:
                        content = chunk.content
                        full_response += content
                        yield {
                            "type": "content",
                            "content": content
                        }
            except Exception as e:
                logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                yield {
                    "type": "error",
                    "content": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                }
                return
            
            # 5. ì¶œì²˜ ì •ë³´ ì¶”ê°€
            source_text = {
                "pdf": "\n\nğŸ“„ *[ì¶œì²˜: ì—…ë¡œë“œëœ ì •ì±… ë¬¸ì„œ]*",
                "web": "\n\nğŸŒ *[ì¶œì²˜: ì›¹ ê²€ìƒ‰ ê²°ê³¼]*"
            }.get(search_source, "")
            
            if source_text:
                yield {
                    "type": "content",
                    "content": source_text
                }
            
            # 6. ì™„ë£Œ ì‹ í˜¸
            yield {
                "type": "done",
                "search_source": search_source,
                "full_response": full_response + source_text
            }
            
            logger.info("ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            yield {
                "type": "error",
                "content": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
graph_service = GraphService()

