"""
LangGraph ê¸°ë°˜ ì²­ë…„ ê¸ˆìœµ/ì£¼íƒ ì •ì±… ì±—ë´‡ ì›Œí¬í”Œë¡œìš°
ChromaDB PDF ê²€ìƒ‰ â†’ ê´€ë ¨ì„± ì²´í¬ â†’ ì›¹ ê²€ìƒ‰ (í•„ìš”ì‹œ) â†’ ë‹µë³€ ìƒì„±
"""

from typing import Annotated, TypedDict, Optional
from langgraph.graph import END, StateGraph
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langchain_upstage import ChatUpstage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from tavily import TavilyClient
from app.config import settings
from app.rag_service import rag_service
import logging
import json

logger = logging.getLogger(__name__)


# GraphState ì •ì˜
class GraphState(TypedDict):
    """ê·¸ë˜í”„ ìƒíƒœ"""
    question: Annotated[str, "Question"]  # ì‚¬ìš©ì ì§ˆë¬¸
    context: Annotated[str, "Context"]  # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸
    answer: Annotated[str, "Answer"]  # ìƒì„±ëœ ë‹µë³€
    messages: Annotated[list, add_messages]  # ëŒ€í™” íˆìŠ¤í† ë¦¬
    relevance: Annotated[str, "Relevance"]  # ê´€ë ¨ì„± ì²´í¬ ê²°ê³¼ (yes/no)
    search_source: Annotated[str, "SearchSource"]  # ì •ë³´ ì¶œì²˜ (pdf/web)
    user_profile: Annotated[dict, "UserProfile"]  # ì‚¬ìš©ì í”„ë¡œí•„
    sources: Annotated[list, "Sources"]  # ì›¹ ê²€ìƒ‰ ì¶œì²˜ (ì œëª©, URL)


# ì²­ë…„ ì •ì±… ì „ë¬¸ í”„ë¡¬í”„íŠ¸
YOUTH_POLICY_PROMPT = ChatPromptTemplate.from_messages([
    (
        "system",
        """ë‹¹ì‹ ì€ ì²­ë…„ ê¸ˆìœµ ë° ì£¼íƒ ì •ì±… ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

**ì—­í• **
- ì²­ë…„ë“¤ì˜ ê¸ˆìœµ ë° ì£¼íƒ ê´€ë ¨ ê³ ë¯¼ì„ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ í•´ê²°í•´ì£¼ì„¸ìš”.
- ë³µì¡í•œ ì •ì±…ì„ ì‰½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
- êµ¬ì²´ì ì¸ ì‹ ì²­ ì¡°ê±´, ì ˆì°¨, í•„ìš” ì„œë¥˜ë¥¼ ì•ˆë‚´í•´ì£¼ì„¸ìš”.

**ë‹µë³€ ì›ì¹™**
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ì •ë³´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.
2. ì‹ ì²­ ìê²©, ëŒ€ì¶œ í•œë„, ê¸ˆë¦¬ ë“± í•µì‹¬ ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ ì•ˆë‚´í•©ë‹ˆë‹¤.
3. ì—¬ëŸ¬ ì •ì±…ì´ ìˆë‹¤ë©´ ë¹„êµí•˜ì—¬ ìµœì ì˜ ì„ íƒì„ ë„ì™€ì¤ë‹ˆë‹¤.
4. ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ì•Šê³  í™•ì¸ì´ í•„ìš”í•˜ë‹¤ê³  ì•ˆë‚´í•©ë‹ˆë‹¤.
5. ì¹œê·¼í•˜ê³  ê³µê°í•˜ëŠ” í†¤ìœ¼ë¡œ ëŒ€í™”í•©ë‹ˆë‹¤.

**ë‹µë³€ í˜•ì‹**
- í•µì‹¬ ë‚´ìš©ì„ ë¨¼ì € ì œì‹œí•˜ê³ , ìƒì„¸ ì •ë³´ë¥¼ ì´ì–´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤.
- ì¡°ê±´ì´ ìˆëŠ” ê²½ìš° ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì—¬ ì„¤ëª…í•©ë‹ˆë‹¤.
- í•„ìš”ì‹œ ë‹¨ê³„ë³„ë¡œ ì •ë¦¬í•˜ì—¬ ì•ˆë‚´í•©ë‹ˆë‹¤.

**ì œê³µëœ ì»¨í…ìŠ¤íŠ¸**
{context}

**ì‚¬ìš©ì í”„ë¡œí•„(ìˆìœ¼ë©´ ë°˜ì˜)**
{user_profile}

**ì´ì „ ëŒ€í™” ë‚´ì—­**
{chat_history}
""",
    ),
    ("human", "{question}"),
])


class GraphService:
    """LangGraph ê¸°ë°˜ ì±—ë´‡ ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.llm = None
        self.youth_policy_chain = None
        self.tavily_client = None
        self.app = None
        self.memory = MemorySaver()
        self._initializing = False
        self._initialized = False
        
        # ì´ˆê¸°í™”ëŠ” ë‚˜ì¤‘ì— (ì„œë²„ ì‹œì‘ í›„)
        # self._initialize()  # ì£¼ì„ ì²˜ë¦¬
    
    def initialize(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ë™ê¸°)"""
        if self._initializing or self._initialized:
            return
        self._initializing = True
        try:
            self._initialize()
            self._initialized = True
            logger.info("GraphService ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"GraphService ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
            self._initialized = False
        finally:
            self._initializing = False
    
    def _initialize(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ë‚´ë¶€ ë¡œì§"""
        try:
            # Upstage Solar LLM ì´ˆê¸°í™” (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
            if settings.upstage_api_key:
                self.llm = ChatUpstage(
                    model=settings.upstage_model,
                    temperature=settings.temperature,
                    api_key=settings.upstage_api_key,
                    streaming=True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
                )
                self.youth_policy_chain = YOUTH_POLICY_PROMPT | self.llm | StrOutputParser()
                logger.info("Upstage Solar LLM ì´ˆê¸°í™” ì™„ë£Œ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)")
            else:
                logger.warning("UPSTAGE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            # Tavily ì›¹ ê²€ìƒ‰ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            if settings.tavily_api_key:
                self.tavily_client = TavilyClient(api_key=settings.tavily_api_key)
                logger.info("Tavily ì›¹ ê²€ìƒ‰ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                logger.warning("TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            # LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì¶•
            self._build_graph()
            
        except Exception as e:
            logger.error(f"GraphService ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _build_graph(self):
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì¶•"""
        if not self.llm:
            logger.warning("LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # ê·¸ë˜í”„ ìƒì„±
        workflow = StateGraph(GraphState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("retrieve", self._retrieve_document)
        workflow.add_node("relevance_check", self._relevance_check)
        workflow.add_node("web_search", self._web_search)
        workflow.add_node("llm_answer", self._llm_answer)
        
        # ì—£ì§€ ì¶”ê°€
        workflow.add_edge("retrieve", "relevance_check")
        workflow.add_conditional_edges(
            "relevance_check",
            self._is_relevant,
            {
                "relevant": "llm_answer",
                "not_relevant": "web_search"
            }
        )
        workflow.add_edge("web_search", "llm_answer")
        workflow.add_edge("llm_answer", END)
        
        # ì§„ì…ì  ì„¤ì •
        workflow.set_entry_point("retrieve")
        
        # ê·¸ë˜í”„ ì»´íŒŒì¼
        self.app = workflow.compile(checkpointer=self.memory)
        logger.info("LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì¶• ì™„ë£Œ")
    
    async def _retrieve_document(self, state: GraphState) -> GraphState:
        """1. PDF ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ (ë¹„ë™ê¸°)"""
        question = state["question"]
        logger.info(f"PDF ë¬¸ì„œ ê²€ìƒ‰: {question[:50]}...")
        
        # RAG ì„œë¹„ìŠ¤ë¡œ ë¬¸ì„œ ê²€ìƒ‰ (ë¹„ë™ê¸°)
        retriever = rag_service.get_retriever()
        if retriever:
            try:
                retrieved_docs = await retriever.ainvoke(question)
                context = rag_service.format_docs(retrieved_docs)
                
                if context:
                    logger.info(f"{len(retrieved_docs)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬")
                else:
                    logger.info("ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•¨")
                
                return GraphState(context=context, search_source="pdf")
            except Exception as e:
                logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                return GraphState(context="", search_source="pdf")
        else:
            logger.warning("Retrieverê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return GraphState(context="", search_source="pdf")
    
    async def _relevance_check(self, state: GraphState) -> GraphState:
        """2. ê´€ë ¨ì„± ì²´í¬ ë…¸ë“œ (LLM ê¸°ë°˜, ë¹„ë™ê¸°)"""
        # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ê´€ë ¨ì„± ì—†ìŒ
        context = state.get("context", "")
        if not context or context.strip() == "":
            logger.info("ê´€ë ¨ì„± ì²´í¬: NO (ë¬¸ì„œ ì—†ìŒ)")
            return GraphState(relevance="no")
        
        question = state["question"]
        
        # LLMì„ ì‚¬ìš©í•œ ì •êµí•œ ê´€ë ¨ì„± ì²´í¬ (ë¹„ë™ê¸°)
        logger.info("ğŸ¤– LLM ê´€ë ¨ì„± ì²´í¬ ì‹œì‘...")
        relevance_prompt = f"""ë‹¹ì‹ ì€ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì§ˆë¬¸: {question}

ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©:
{context[:1000]}

ìœ„ ë¬¸ì„œê°€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ë° ìœ ìš©í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆê¹Œ?

ê·œì¹™:
- ë¬¸ì„œ ë‚´ìš©ì´ ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆìœ¼ë©´ "YES"
- ë¬¸ì„œ ë‚´ìš©ì´ ì§ˆë¬¸ê³¼ ì „í˜€ ê´€ë ¨ì´ ì—†ìœ¼ë©´ "NO"
- ë‹¨ìˆœíˆ í‚¤ì›Œë“œê°€ ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì‹¤ì§ˆì ìœ¼ë¡œ ë‹µë³€ì— ë„ì›€ì´ ë˜ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”

ë‹µë³€ì€ ë°˜ë“œì‹œ "YES" ë˜ëŠ” "NO" ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        try:
            response = await self.llm.ainvoke(relevance_prompt)
            result = response.content.strip().upper()
            
            relevance = "yes" if "YES" in result else "no"
            logger.info(f"âœ… ê´€ë ¨ì„± ì²´í¬ ì™„ë£Œ: {relevance.upper()} (LLM íŒë‹¨: {result[:20]})")
            
        except Exception as e:
            logger.error(f"ê´€ë ¨ì„± ì²´í¬ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ 'yes' ì‚¬ìš©")
            relevance = "yes"
        
        return GraphState(relevance=relevance)
    
    async def _web_search(self, state: GraphState) -> GraphState:
        """3. ì›¹ ê²€ìƒ‰ ë…¸ë“œ (ë¹„ë™ê¸°)"""
        question = state["question"]
        logger.info(f"ì›¹ ê²€ìƒ‰: {question[:50]}...")
        
        if not self.tavily_client:
            logger.warning("Tavily í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return GraphState(
                context="ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                search_source="web"
            )
        
        try:
            # ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” (ì²­ë…„ ì •ì±… í‚¤ì›Œë“œ ì¶”ê°€)
            enhanced_query = f"ì²­ë…„ {question}" if "ì²­ë…„" not in question else question

            # Tavily ê²€ìƒ‰ ìˆ˜í–‰ (ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰)
            import asyncio
            loop = asyncio.get_event_loop()
            search_results = await loop.run_in_executor(
                None,
                lambda: self.tavily_client.search(query=enhanced_query, max_results=5)
            )

            # ê²°ê³¼ í¬ë§·íŒ… ë° ì¶œì²˜ URL ì €ì¥
            context = ""
            sources = []
            if search_results and "results" in search_results:
                for result in search_results["results"][:3]:
                    context += f"{result.get('content', '')}\n\n"
                    # ì¶œì²˜ ì •ë³´ ì €ì¥
                    sources.append({
                        "title": result.get('title', 'Untitled'),
                        "url": result.get('url', ''),
                        "score": result.get('score', 0)
                    })

            logger.info(f"ì›¹ ê²€ìƒ‰ ì™„ë£Œ (ì¶œì²˜ {len(sources)}ê°œ)")
            return GraphState(context=context, search_source="web", sources=sources)
            
        except Exception as e:
            logger.error(f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return GraphState(
                context=f"ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                search_source="web"
            )
    
    async def _llm_answer(self, state: GraphState) -> GraphState:
        """4. ë‹µë³€ ìƒì„± ë…¸ë“œ (ë¹„ë™ê¸°)"""
        question = state["question"]
        context = state.get("context", "")
        
        logger.info("ë‹µë³€ ìƒì„± ì¤‘...")
        
        if not self.youth_policy_chain:
            return GraphState(
                answer="AI ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                messages=[("user", question), ("assistant", "ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì˜¤ë¥˜")]
            )
        
        try:
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
            chat_history = ""
            messages = state.get("messages", [])
            if messages:
                for msg in messages[-6:]:  # ìµœê·¼ 3í„´
                    if isinstance(msg, tuple) and len(msg) == 2:
                        role, content = msg
                        chat_history += f"{role}: {content}\n"
                    elif hasattr(msg, 'type') and hasattr(msg, 'content'):
                        chat_history += f"{msg.type}: {msg.content}\n"
            
            # ë‹µë³€ ìƒì„± (ë¹„ë™ê¸°)
            response = await self.youth_policy_chain.ainvoke({
                "question": question,
                "context": context,
                "chat_history": chat_history,
                "user_profile": state.get("user_profile", {})
            })
            
            # ì •ë³´ ì¶œì²˜ ì•ˆë‚´ ì¶”ê°€
            source = state.get("search_source", "unknown")
            source_text = {
                "pdf": "\n\nğŸ“„ *[ì¶œì²˜: ì—…ë¡œë“œëœ ì •ì±… ë¬¸ì„œ]*",
                "web": "\n\nğŸŒ *[ì¶œì²˜: ì›¹ ê²€ìƒ‰ ê²°ê³¼ - ìµœì‹  ì •ë³´ì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ê³µì‹ ì‚¬ì´íŠ¸ì—ì„œ í™•ì¸ì„ ê¶Œì¥í•©ë‹ˆë‹¤]*"
            }.get(source, "")
            
            final_answer = f"{response}{source_text}"
            
            logger.info("ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
            return GraphState(
                answer=final_answer,
                messages=[("user", question), ("assistant", final_answer)]
            )
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            error_msg = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            return GraphState(
                answer=error_msg,
                messages=[("user", question), ("assistant", error_msg)]
            )
    
    def _is_relevant(self, state: GraphState) -> str:
        """ê´€ë ¨ì„± ë¼ìš°íŒ… í•¨ìˆ˜"""
        return "relevant" if state.get("relevance") == "yes" else "not_relevant"
    
    async def ask(
        self,
        question: str,
        thread_id: str,
        user_profile: Optional[dict] = None
    ) -> dict:
        """
        ì§ˆë¬¸í•˜ê³  ë‹µë³€ ë°›ê¸°
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            thread_id: ëŒ€í™” ì„¸ì…˜ ID
            user_profile: ì‚¬ìš©ì í”„ë¡œí•„ (ì„ íƒ)
            
        Returns:
            ë‹µë³€ ë° ìƒíƒœ ì •ë³´
        """
        if not self.app:
            return {
                "answer": "AI ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. UPSTAGE_API_KEYë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
                "search_source": "error"
            }
        
        try:
            # ì…ë ¥ ì¤€ë¹„
            inputs = GraphState(
                question=question,
                user_profile=(user_profile or {})
            )
            
            # ì„¤ì •
            config = {"configurable": {"thread_id": thread_id}}
            
            # ì‹¤í–‰
            result = await self.app.ainvoke(inputs, config)
            
            return {
                "answer": result.get("answer", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."),
                "search_source": result.get("search_source", "unknown"),
                "context": result.get("context", "")
            }
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "answer": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "search_source": "error"
            }
    
    async def stream_ask(
        self,
        question: str,
        thread_id: str,
        user_profile: Optional[dict] = None
    ):
        """
        ì§ˆë¬¸í•˜ê³  ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹µë³€ ë°›ê¸°
        
        LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° 
        - ëª¨ë“  ë…¸ë“œë¥¼ ë¹„ë™ê¸°ë¡œ ìµœì í™”í•˜ì—¬ ì„±ëŠ¥ ê°œì„ 
        - llm_answer ë…¸ë“œì—ì„œ ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            thread_id: ëŒ€í™” ì„¸ì…˜ ID
            user_profile: ì‚¬ìš©ì í”„ë¡œí•„ (ì„ íƒ)
            
        Yields:
            ë‹µë³€ ì²­í¬ ë° ë©”íƒ€ë°ì´í„°
        """
        if not self.app:
            yield {
                "type": "error",
                "content": "AI ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
            return
        
        try:
            logger.info(f"ìŠ¤íŠ¸ë¦¬ë° ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘ (LangGraph ì‚¬ìš©): {question[:50]}...")
            
            # ì…ë ¥ ì¤€ë¹„
            inputs = GraphState(
                question=question,
                user_profile=(user_profile or {})
            )
            
            # ì„¤ì •
            config = {"configurable": {"thread_id": thread_id}}
            
            # LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            # í•˜ì§€ë§Œ llm_answer ë…¸ë“œ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³ , ê´€ë ¨ì„± ì²´í¬ ì™„ë£Œ í›„ ì¦‰ì‹œ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
            full_answer = ""
            search_source = "unknown"
            first_content_received = False
            context = ""
            relevance = "yes"
            streaming_started = False
            sources = []  # ì›¹ ê²€ìƒ‰ ì¶œì²˜ ì €ì¥
            
            # LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•˜ì—¬ í•„ìš”í•œ ì •ë³´ ìˆ˜ì§‘
            async for event in self.app.astream(inputs, config):
                # ê° ë…¸ë“œì˜ ì´ë²¤íŠ¸ ì²˜ë¦¬
                for node_name, node_output in event.items():
                    if node_name == "retrieve":
                        yield {"type": "status", "content": "ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."}
                        context = node_output.get("context", "")
                        search_source = node_output.get("search_source", "unknown")
                        
                    elif node_name == "relevance_check":
                        yield {"type": "status", "content": "ê´€ë ¨ì„± ê²€ì‚¬ ì¤‘..."}
                        relevance = node_output.get("relevance", "yes")
                        
                        yield {
                            "type": "metadata",
                            "relevance_check_completed": True,
                            "relevance": relevance
                        }
                        
                        # í•µì‹¬ ìµœì í™”: ê´€ë ¨ì„± ì²´í¬ ì™„ë£Œ í›„ ì¦‰ì‹œ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘!
                        # llm_answer ë…¸ë“œ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ì§€ ì•ŠìŒ
                        if not streaming_started and relevance == "yes":
                            streaming_started = True
                            
                            # ë‹µë³€ ìƒì„± ì‹œì‘
                            yield {"type": "status", "content": "ë‹µë³€ ìƒì„± ì¤‘..."}
                            
                            # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
                            chat_history = ""
                            
                            # í”„ë¡¬í”„íŠ¸ ìƒì„±
                            user_profile_text = ""
                            if user_profile:
                                user_profile_text = f"\nì‚¬ìš©ì í”„ë¡œí•„: {json.dumps(user_profile, ensure_ascii=False)}"
                            
                            chain_input = {
                                "question": question,
                                "context": context if context else "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                                "chat_history": chat_history,
                                "user_profile": user_profile_text
                            }
                            
                            messages = YOUTH_POLICY_PROMPT.format_messages(**chain_input)
                            
                            yield {
                                "type": "metadata",
                                "answer_generation_started": True,
                                "search_source": search_source,
                                "context_length": len(context)
                            }
                            
                            # LLM ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± (ì¦‰ì‹œ ì‹œì‘!)
                            try:
                                async for chunk in self.llm.astream(messages):
                                    if hasattr(chunk, 'content') and chunk.content:
                                        content = chunk.content
                                        full_answer += content
                                        
                                        if not first_content_received:
                                            first_content_received = True
                                            yield {
                                                "type": "metadata",
                                                "llm_streaming_started": True
                                            }
                                        
                                        yield {
                                            "type": "content",
                                            "content": content
                                        }
                            except Exception as e:
                                logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                                yield {
                                    "type": "error",
                                    "content": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                                }
                                return
                            
                    elif node_name == "web_search":
                        yield {"type": "status", "content": "ì›¹ ê²€ìƒ‰ ì¤‘..."}
                        context = node_output.get("context", "")
                        search_source = node_output.get("search_source", "web")
                        sources = node_output.get("sources", [])

                        logger.info(f"ğŸ” ì›¹ ê²€ìƒ‰ ì™„ë£Œ: sources ê°œìˆ˜ = {len(sources)}")
                        if sources:
                            logger.info(f"ğŸ“¤ Sources ì „ì†¡: {sources}")

                        # ì›¹ ê²€ìƒ‰ ì¶œì²˜ ì •ë³´ ì „ì†¡
                        if sources:
                            yield {
                                "type": "sources",
                                "sources": sources
                            }
                            logger.info(f"âœ… Sources yield ì™„ë£Œ")

                        # ì›¹ ê²€ìƒ‰ ì™„ë£Œ í›„ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
                        if not streaming_started:
                            streaming_started = True
                            
                            # ë‹µë³€ ìƒì„± ì‹œì‘
                            yield {"type": "status", "content": "ë‹µë³€ ìƒì„± ì¤‘..."}
                            
                            # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
                            chat_history = ""
                            
                            # í”„ë¡¬í”„íŠ¸ ìƒì„±
                            user_profile_text = ""
                            if user_profile:
                                user_profile_text = f"\nì‚¬ìš©ì í”„ë¡œí•„: {json.dumps(user_profile, ensure_ascii=False)}"
                            
                            chain_input = {
                                "question": question,
                                "context": context if context else "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                                "chat_history": chat_history,
                                "user_profile": user_profile_text
                            }
                            
                            messages = YOUTH_POLICY_PROMPT.format_messages(**chain_input)
                            
                            yield {
                                "type": "metadata",
                                "answer_generation_started": True,
                                "search_source": search_source,
                                "context_length": len(context)
                            }
                            
                            # LLM ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
                            try:
                                async for chunk in self.llm.astream(messages):
                                    if hasattr(chunk, 'content') and chunk.content:
                                        content = chunk.content
                                        full_answer += content
                                        
                                        if not first_content_received:
                                            first_content_received = True
                                            yield {
                                                "type": "metadata",
                                                "llm_streaming_started": True
                                            }
                                        
                                        yield {
                                            "type": "content",
                                            "content": content
                                        }
                            except Exception as e:
                                logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                                yield {
                                    "type": "error",
                                    "content": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                                }
                                return
                        
                    elif node_name == "llm_answer":
                        # llm_answer ë…¸ë“œëŠ” ì´ë¯¸ ìŠ¤íŠ¸ë¦¬ë°ì´ ì‹œì‘ëœ í›„ì´ë¯€ë¡œ
                        # ì—¬ê¸°ì„œëŠ” ì™„ë£Œ í™•ì¸ë§Œ ìˆ˜í–‰
                        pass
            
            # ìŠ¤íŠ¸ë¦¬ë°ì´ ì‹œì‘ë˜ì§€ ì•Šì€ ê²½ìš° (fallback)
            if not streaming_started:
                # ì¼ë°˜ì ì¸ ê²½ìš°: llm_answer ë…¸ë“œ ì™„ë£Œ í›„ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
                yield {"type": "status", "content": "ë‹µë³€ ìƒì„± ì¤‘..."}
                
                # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
                chat_history = ""
                
                # í”„ë¡¬í”„íŠ¸ ìƒì„±
                user_profile_text = ""
                if user_profile:
                    user_profile_text = f"\nì‚¬ìš©ì í”„ë¡œí•„: {json.dumps(user_profile, ensure_ascii=False)}"
                
                chain_input = {
                    "question": question,
                    "context": context if context else "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "chat_history": chat_history,
                    "user_profile": user_profile_text
                }
                
                messages = YOUTH_POLICY_PROMPT.format_messages(**chain_input)
                
                yield {
                    "type": "metadata",
                    "answer_generation_started": True,
                    "search_source": search_source,
                    "context_length": len(context)
                }
                
                # LLM ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
                try:
                    async for chunk in self.llm.astream(messages):
                        if hasattr(chunk, 'content') and chunk.content:
                            content = chunk.content
                            full_answer += content
                            
                            if not first_content_received:
                                first_content_received = True
                                yield {
                                    "type": "metadata",
                                    "llm_streaming_started": True
                                }
                            
                            yield {
                                "type": "content",
                                "content": content
                            }
                except Exception as e:
                    logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                    yield {
                        "type": "error",
                        "content": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                    }
                    return
            
            # ì¶œì²˜ ì •ë³´ ì¶”ê°€
            source_text = {
                "pdf": "\n\nğŸ“„ *[ì¶œì²˜: ì—…ë¡œë“œëœ ì •ì±… ë¬¸ì„œ]*",
                "web": "\n\nğŸŒ *[ì¶œì²˜: ì›¹ ê²€ìƒ‰ ê²°ê³¼ - ìµœì‹  ì •ë³´ì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ê³µì‹ ì‚¬ì´íŠ¸ì—ì„œ í™•ì¸ì„ ê¶Œì¥í•©ë‹ˆë‹¤]*"
            }.get(search_source, "")
            
            if source_text:
                yield {
                    "type": "content",
                    "content": source_text
                }
            
            full_answer += source_text
            
            # ë©”ëª¨ë¦¬ì— ë‹µë³€ ì €ì¥ (ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸)
            try:
                from langchain_core.messages import HumanMessage, AIMessage
                self.memory.put(
                    config,
                    {
                        "values": {
                            "messages": [
                                HumanMessage(content=question),
                                AIMessage(content=full_answer)
                            ]
                        }
                    }
                )
            except:
                pass  # ë©”ëª¨ë¦¬ ì €ì¥ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ
            
            # ì™„ë£Œ ì‹ í˜¸
            done_event = {
                "type": "done",
                "search_source": search_source,
                "full_response": full_answer
            }

            # ì›¹ ê²€ìƒ‰ ì¶œì²˜ê°€ ìˆìœ¼ë©´ í¬í•¨
            if sources:
                done_event["sources"] = sources

            yield done_event
            
            logger.info("ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± ì™„ë£Œ (LangGraph ì‚¬ìš©)")
            
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            yield {
                "type": "error",
                "content": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
graph_service = GraphService()

